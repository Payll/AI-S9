{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe8b3443ff4abb98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/AI/Data/merged_data.csv /content/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5752cbe70841ac09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = 'merged_data.csv'\n",
    "\n",
    "# Load the DataFrame from the CSV file\n",
    "df_merged = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "# Determine the split index\n",
    "split_index = int(0.8 * len(df_merged))\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_df = df_merged[:split_index]\n",
    "test_df = df_merged[split_index:]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}, Testing set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convertir_en_liste(chaine):\n",
    "    try:\n",
    "        # Convertit la chaîne en liste en utilisant ast.literal_eval\n",
    "        return ast.literal_eval(chaine)\n",
    "    except Exception as e:\n",
    "        # Gérer les exceptions si la conversion échoue\n",
    "        print(f\"Erreur de conversion : {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def reduire_pad(liste):\n",
    "    nouvelle_liste = []\n",
    "    for elem in liste:\n",
    "        if elem != \"[PAD]\" or (nouvelle_liste and nouvelle_liste[-1] != \"[PAD]\"):\n",
    "            nouvelle_liste.append(elem)\n",
    "    return nouvelle_liste\n",
    "\n",
    "\n",
    "# Appliquer la conversion et la réduction\n",
    "# Assurez-vous que train_df est une copie indépendante si c'est un sous-ensemble\n",
    "train_df = train_df.copy()\n",
    "\n",
    "# Appliquer la conversion et la réduction en utilisant .loc\n",
    "train_df.loc[:, 'phoneme'] = train_df['phoneme'].apply(lambda x: reduire_pad(convertir_en_liste(x)))\n",
    "\n",
    "# Afficher les premières lignes de train_df pour vérifier les changements\n",
    "print(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a790f55b298ad26d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_df = test_df.copy()\n",
    "\n",
    "# Appliquer la conversion et la réduction en utilisant .loc\n",
    "test_df.loc[:, 'phoneme'] = test_df['phoneme'].apply(lambda x: reduire_pad(convertir_en_liste(x)))\n",
    "\n",
    "# Afficher les premières lignes de train_df pour vérifier les changements\n",
    "print(test_df.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d68ccf6a7815e957"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df['language'] = train_df['language'].apply(lambda x: 0 if x == 'French' else 1)\n",
    "test_df['language'] = test_df['language'].apply(lambda x: 0 if x == 'French' else 1)\n",
    "\n",
    "# Ensuite, continuez avec la tokenisation et le padding comme avant\n",
    "phoneme_tokenizer = Tokenizer(char_level=False)\n",
    "phoneme_tokenizer.fit_on_texts(train_df['phoneme'])\n",
    "X_train = phoneme_tokenizer.texts_to_sequences(train_df['phoneme'])\n",
    "X_train = pad_sequences(X_train, padding='post')\n",
    "\n",
    "X_test = phoneme_tokenizer.texts_to_sequences(test_df['phoneme'])\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=X_train.shape[1])\n",
    "\n",
    "# Encode the language labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['language'])\n",
    "y_test = label_encoder.transform(test_df['language'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa656821916029b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(test_df.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52f7289dbbb3955a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(train_df.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef26baef7f5dacac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters for the model\n",
    "vocab_size = len(phoneme_tokenizer.word_index) + 1  # Vocabulary size\n",
    "embed_dim = 1024  # Dimension of the embedding vectors\n",
    "max_length = X_train.shape[1]  # Maximum length of the input sequences\n",
    "output_dim = len(label_encoder.classes_)  # Number of unique output classes\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_length),\n",
    "    LSTM(units=128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    LSTM(units=128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc4f48e290621f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test), batch_size=32,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Préparation de la gamme des époques pour le tracé.\n",
    "epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "# Configuration de la taille et du layout des graphiques.\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Tracé de la perte d'entraînement et de validation.\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Tracé de la précision d'entraînement et de validation.\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Affichage des graphiques.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0d2d61e5e9599cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad6131f79a64c5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
